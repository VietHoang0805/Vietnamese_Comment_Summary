{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57c7e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë l∆∞·ª£ng posts: 141\n"
     ]
    }
   ],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn file XML\n",
    "xml_path = \"Dataset/VSoSLCSum.xml\"\n",
    "\n",
    "# Parse file XML\n",
    "tree = ET.parse(xml_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng posts: {len(root.findall('post'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "957a690b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ tr√≠ch xu·∫•t 141 posts\n",
      "\n",
      "--- M·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n ---\n",
      "Post ID: yen-bai_vne_2\n",
      "Title: T√¨nh_c·∫£m c·ªßa h√†ng_x√≥m v·ªõi B√≠_th∆∞ T·ªânh_·ªßy Y√™n_B√°i\n",
      "S·ªë c√¢u docs: 26\n",
      "S·ªë c√¢u comments: 29\n",
      "Selected docs indices: [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 20, 24]\n",
      "Selected comment indices: [0, 1, 6, 7, 9, 11, 13, 14, 15, 17, 21, 23, 24, 25]\n"
     ]
    }
   ],
   "source": [
    "# H√†m tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ XML\n",
    "def extract_data_from_xml(root):\n",
    "    \"\"\"\n",
    "    Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ XML v·ªõi c·∫•u tr√∫c:\n",
    "    - title: Ti√™u ƒë·ªÅ b√†i vi·∫øt\n",
    "    - content_docs: T·∫•t c·∫£ c√°c c√¢u t·ª´ document (ƒë√°nh s·ªë)\n",
    "    - content_comment: T·∫•t c·∫£ c√°c c√¢u t·ª´ comments (ƒë√°nh s·ªë)\n",
    "    - golden_docs: C√°c c√¢u document ƒë∆∞·ª£c ch·ªçn v√†o summary\n",
    "    - golden_comment: C√°c c√¢u comment ƒë∆∞·ª£c ch·ªçn v√†o summary\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for post in root.findall('post'):\n",
    "        post_id = post.get('id')\n",
    "        \n",
    "        # L·∫•y title\n",
    "        title_elem = post.find('title')\n",
    "        title = title_elem.text.strip() if title_elem is not None and title_elem.text else \"\"\n",
    "        \n",
    "        # L·∫•y summary (golden summary) - gi·ªØ l·∫°i ƒë·ªÉ tham kh·∫£o\n",
    "        summary_sentences = []\n",
    "        summary_elem = post.find('summary/sentences')\n",
    "        if summary_elem is not None:\n",
    "            for sentence in summary_elem.findall('sentence'):\n",
    "                content_elem = sentence.find('content')\n",
    "                if content_elem is not None and content_elem.text:\n",
    "                    summary_sentences.append(content_elem.text.strip())\n",
    "        golden_full = \" \".join(summary_sentences)\n",
    "        \n",
    "        # L·∫•y document sentences\n",
    "        doc_sentences = []\n",
    "        doc_elem = post.find('document/sentences')\n",
    "        if doc_elem is not None:\n",
    "            for idx, sentence in enumerate(doc_elem.findall('sentence')):\n",
    "                content_elem = sentence.find('content')\n",
    "                in_summary = sentence.get('in_summary', '0')\n",
    "                if content_elem is not None and content_elem.text:\n",
    "                    doc_sentences.append({\n",
    "                        'index': idx,\n",
    "                        'content': content_elem.text.strip(),\n",
    "                        'in_summary': int(in_summary) if in_summary.isdigit() else 0\n",
    "                    })\n",
    "        \n",
    "        # L·∫•y comment sentences\n",
    "        comment_sentences = []\n",
    "        comments_elem = post.find('comments/comment/sentences')\n",
    "        if comments_elem is not None:\n",
    "            for idx, sentence in enumerate(comments_elem.findall('sentence')):\n",
    "                content_elem = sentence.find('content')\n",
    "                in_summary = sentence.get('in_summary', '0')\n",
    "                if content_elem is not None and content_elem.text:\n",
    "                    comment_sentences.append({\n",
    "                        'index': idx,\n",
    "                        'content': content_elem.text.strip(),\n",
    "                        'in_summary': int(in_summary) if in_summary.isdigit() else 0\n",
    "                    })\n",
    "        \n",
    "        # T·∫°o content_docs string (ƒë√°nh s·ªë c√°c c√¢u document)\n",
    "        content_docs_list = [f\"{i}: {sent['content']}\" for i, sent in enumerate(doc_sentences)]\n",
    "        content_docs = \"\\n\".join(content_docs_list)\n",
    "        \n",
    "        # T·∫°o content_comment string (ƒë√°nh s·ªë c√°c c√¢u comment)\n",
    "        content_comment_list = [f\"{i}: {sent['content']}\" for i, sent in enumerate(comment_sentences)]\n",
    "        content_comment = \"\\n\".join(content_comment_list)\n",
    "        \n",
    "        # L·∫•y golden_docs: c√°c c√¢u document ƒë∆∞·ª£c ch·ªçn v√†o summary (in_summary >= 1)\n",
    "        golden_docs_sentences = [sent['content'] for sent in doc_sentences if sent['in_summary'] >= 1]\n",
    "        golden_docs = \" \".join(golden_docs_sentences)\n",
    "        \n",
    "        # L·∫•y golden_comment: c√°c c√¢u comment ƒë∆∞·ª£c ch·ªçn v√†o summary (in_summary >= 1)\n",
    "        golden_comment_sentences = [sent['content'] for sent in comment_sentences if sent['in_summary'] >= 1]\n",
    "        golden_comment = \" \".join(golden_comment_sentences)\n",
    "        \n",
    "        # L·∫•y c√°c index c·ªßa c√¢u ƒë∆∞·ª£c ch·ªçn\n",
    "        selected_docs_indices = [sent['index'] for sent in doc_sentences if sent['in_summary'] >= 1]\n",
    "        selected_comment_indices = [sent['index'] for sent in comment_sentences if sent['in_summary'] >= 1]\n",
    "        \n",
    "        data.append({\n",
    "            'post_id': post_id,\n",
    "            'title': title,\n",
    "            'content_docs': content_docs,\n",
    "            'content_comment': content_comment,\n",
    "            'golden_docs': golden_docs,\n",
    "            'golden_comment': golden_comment,\n",
    "            'golden_full': golden_full,\n",
    "            'selected_docs_indices': selected_docs_indices,\n",
    "            'selected_comment_indices': selected_comment_indices,\n",
    "            'num_docs': len(doc_sentences),\n",
    "            'num_comments': len(comment_sentences)\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Tr√≠ch xu·∫•t d·ªØ li·ªáu\n",
    "data = extract_data_from_xml(root)\n",
    "print(f\"ƒê√£ tr√≠ch xu·∫•t {len(data)} posts\")\n",
    "\n",
    "# Xem m·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n\n",
    "if data:\n",
    "    print(\"\\n--- M·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n ---\")\n",
    "    print(f\"Post ID: {data[0]['post_id']}\")\n",
    "    print(f\"Title: {data[0]['title']}\")\n",
    "    print(f\"S·ªë c√¢u docs: {data[0]['num_docs']}\")\n",
    "    print(f\"S·ªë c√¢u comments: {data[0]['num_comments']}\")\n",
    "    print(f\"Selected docs indices: {data[0]['selected_docs_indices']}\")\n",
    "    print(f\"Selected comment indices: {data[0]['selected_comment_indices']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c1978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Th√¥ng tin DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 141 entries, 0 to 140\n",
      "Data columns (total 11 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   post_id                   141 non-null    object\n",
      " 1   title                     141 non-null    object\n",
      " 2   content_docs              141 non-null    object\n",
      " 3   content_comment           141 non-null    object\n",
      " 4   golden_docs               141 non-null    object\n",
      " 5   golden_comment            141 non-null    object\n",
      " 6   golden_full               141 non-null    object\n",
      " 7   selected_docs_indices     141 non-null    object\n",
      " 8   selected_comment_indices  141 non-null    object\n",
      " 9   num_docs                  141 non-null    int64 \n",
      " 10  num_comments              141 non-null    int64 \n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 12.2+ KB\n",
      "None\n",
      "\n",
      "S·ªë d√≤ng: 141\n",
      "\n",
      "C√°c c·ªôt: ['post_id', 'title', 'content_docs', 'content_comment', 'golden_docs', 'golden_comment', 'golden_full', 'selected_docs_indices', 'selected_comment_indices', 'num_docs', 'num_comments']\n",
      "\n",
      "‚úÖ ƒê√£ c√≥ ƒë·ªß c√°c c·ªôt m·ªõi!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>content_docs</th>\n",
       "      <th>content_comment</th>\n",
       "      <th>golden_docs</th>\n",
       "      <th>golden_comment</th>\n",
       "      <th>golden_full</th>\n",
       "      <th>selected_docs_indices</th>\n",
       "      <th>selected_comment_indices</th>\n",
       "      <th>num_docs</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yen-bai_vne_2</td>\n",
       "      <td>T√¨nh_c·∫£m c·ªßa h√†ng_x√≥m v·ªõi B√≠_th∆∞ T·ªânh_·ªßy Y√™n_B√°i</td>\n",
       "      <td>0: S√°ng 19/8 , th√†nh_ph·ªë Y√™n_B√°i m∆∞a kh√¥ng ng·ªõ...</td>\n",
       "      <td>0: T·∫°i_sao nh∆∞_v·∫≠y ? Ph·∫£i c√≥ nguy√™n_nh√¢n , Ch√∫...</td>\n",
       "      <td>S√°ng 19/8 , th√†nh_ph·ªë Y√™n_B√°i m∆∞a kh√¥ng ng·ªõt d...</td>\n",
       "      <td>T·∫°i_sao nh∆∞_v·∫≠y ? Ph·∫£i c√≥ nguy√™n_nh√¢n , Ch√∫ng_...</td>\n",
       "      <td>B√≠_th∆∞ T·ªânh_u·ª∑ Y√™n_B√°i_Ph·∫°m_Duy_C∆∞·ªùng ƒë∆∞·ª£c h√†n...</td>\n",
       "      <td>[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[0, 1, 6, 7, 9, 11, 13, 14, 15, 17, 21, 23, 24...</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tan-son-nhat_zing_7</td>\n",
       "      <td>L√£nh_ƒë·∫°o s√¢n_bay T√¢n_S∆°n_Nh·∫•t ph·ªß_nh·∫≠n chuy·ªán ...</td>\n",
       "      <td>0: Chi·ªÅu 26/8 , m∆∞a to k√©o_d√†i khi·∫øn nhi·ªÅu n∆°i...</td>\n",
       "      <td>0: B·∫øn du_thuy·ªÅn T√ÇN_S∆†N_NH·∫§T m·ªõi khai_tr∆∞∆°ng ...</td>\n",
       "      <td>Chi·ªÅu 26/8 , m∆∞a to k√©o_d√†i khi·∫øn nhi·ªÅu n∆°i ·ªü ...</td>\n",
       "      <td>B·∫øn du_thuy·ªÅn T√ÇN_S∆†N_NH·∫§T m·ªõi khai_tr∆∞∆°ng . ....</td>\n",
       "      <td>C∆°n m∆∞a l·ªõn chi·ªÅu 26/8 l√†m s√¢n_bay T√¢n_S∆°n_Nh·∫•...</td>\n",
       "      <td>[0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[0, 1, 6, 10, 11, 12, 16, 21, 23, 24, 25, 26, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>donald-trump_vne_52</td>\n",
       "      <td>Donald_Trump n√≥i T·ªïng_th·ªëng Obama l√† ' ng∆∞·ªùi s...</td>\n",
       "      <td>0: ·ª®ng_vi√™n t·ªïng_th·ªëng ƒë·∫£ng C·ªông_h√≤a Donald_Tr...</td>\n",
       "      <td>0: gi·ªù th√¨ t√¥i kh√¥ng c√≤n tin √¥ng ·∫•y s·∫Ω th·∫Øng ,...</td>\n",
       "      <td>·ª®ng_vi√™n t·ªïng_th·ªëng ƒë·∫£ng C·ªông_h√≤a Donald_Trump...</td>\n",
       "      <td>gi·ªù th√¨ t√¥i kh√¥ng c√≤n tin √¥ng ·∫•y s·∫Ω th·∫Øng , nh...</td>\n",
       "      <td>Donald_Trump c√°o_bu·ªôc T·ªïng_th·ªëng M·ªπ_Barack_Oba...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 8, 9]</td>\n",
       "      <td>[0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 1...</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>michael-phelps_zing_6</td>\n",
       "      <td>Gi√†nh HCV th·ª© 21 , Michael_Phelps h·ªìi_sinh</td>\n",
       "      <td>0: ·ªû n·ªôi_dung s·ªü_tr∆∞·ªùng 200 m b∆∞·ªõm , Phelps - ...</td>\n",
       "      <td>0: Qua 5 m√πa olympic m√† th√†nh_t√≠ch v·∫´n s√°ng ch...</td>\n",
       "      <td>·ªû n·ªôi_dung s·ªü_tr∆∞·ªùng 200 m b∆∞·ªõm , Phelps - ƒë√£ ...</td>\n",
       "      <td>Qua 5 m√πa olympic m√† th√†nh_t√≠ch v·∫´n s√°ng ch√≥i ...</td>\n",
       "      <td>Si√™u k√¨nh_ng∆∞ Michael_Phelps ti·∫øp_t·ª•c cho th·∫•y...</td>\n",
       "      <td>[0, 2, 3, 5, 6, 7, 8, 9, 10, 12, 16, 17, 18, 2...</td>\n",
       "      <td>[0, 1, 3, 4, 6, 7, 9, 10, 11, 12, 13, 14, 17, ...</td>\n",
       "      <td>24</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>casa_dantri_3</td>\n",
       "      <td>Danh_s√°ch 9 c√°n_b·ªô chi·∫øn_sƒ© tr√™n m√°y_bay CASA-212</td>\n",
       "      <td>0: L√™_Ki√™m_To√†n - ƒê·∫°i_t√° L·ªØ_ƒëo√†n tr∆∞·ªüng L·ªØ_ƒëo√†...</td>\n",
       "      <td>0: H√£y tr·ªü_v·ªÅ th√¥i c√°c anh !\\n1: c·∫ßu_mong c√°c ...</td>\n",
       "      <td>L√™_Ki√™m_To√†n - ƒê·∫°i_t√° L·ªØ_ƒëo√†n tr∆∞·ªüng L·ªØ_ƒëo√†n 9...</td>\n",
       "      <td>H√£y tr·ªü_v·ªÅ th√¥i c√°c anh ! c·∫ßu_mong c√°c anh tr·ªü...</td>\n",
       "      <td>D√¢n_tr√≠ Chi·∫øc m√°y_bay CASA ch·ªü 9 ng∆∞·ªùi ƒë√£ r∆°i ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 7, 10, 12, 13, 14, 15, 16]</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 post_id                                              title  \\\n",
       "0          yen-bai_vne_2   T√¨nh_c·∫£m c·ªßa h√†ng_x√≥m v·ªõi B√≠_th∆∞ T·ªânh_·ªßy Y√™n_B√°i   \n",
       "1    tan-son-nhat_zing_7  L√£nh_ƒë·∫°o s√¢n_bay T√¢n_S∆°n_Nh·∫•t ph·ªß_nh·∫≠n chuy·ªán ...   \n",
       "2    donald-trump_vne_52  Donald_Trump n√≥i T·ªïng_th·ªëng Obama l√† ' ng∆∞·ªùi s...   \n",
       "3  michael-phelps_zing_6         Gi√†nh HCV th·ª© 21 , Michael_Phelps h·ªìi_sinh   \n",
       "4          casa_dantri_3  Danh_s√°ch 9 c√°n_b·ªô chi·∫øn_sƒ© tr√™n m√°y_bay CASA-212   \n",
       "\n",
       "                                        content_docs  \\\n",
       "0  0: S√°ng 19/8 , th√†nh_ph·ªë Y√™n_B√°i m∆∞a kh√¥ng ng·ªõ...   \n",
       "1  0: Chi·ªÅu 26/8 , m∆∞a to k√©o_d√†i khi·∫øn nhi·ªÅu n∆°i...   \n",
       "2  0: ·ª®ng_vi√™n t·ªïng_th·ªëng ƒë·∫£ng C·ªông_h√≤a Donald_Tr...   \n",
       "3  0: ·ªû n·ªôi_dung s·ªü_tr∆∞·ªùng 200 m b∆∞·ªõm , Phelps - ...   \n",
       "4  0: L√™_Ki√™m_To√†n - ƒê·∫°i_t√° L·ªØ_ƒëo√†n tr∆∞·ªüng L·ªØ_ƒëo√†...   \n",
       "\n",
       "                                     content_comment  \\\n",
       "0  0: T·∫°i_sao nh∆∞_v·∫≠y ? Ph·∫£i c√≥ nguy√™n_nh√¢n , Ch√∫...   \n",
       "1  0: B·∫øn du_thuy·ªÅn T√ÇN_S∆†N_NH·∫§T m·ªõi khai_tr∆∞∆°ng ...   \n",
       "2  0: gi·ªù th√¨ t√¥i kh√¥ng c√≤n tin √¥ng ·∫•y s·∫Ω th·∫Øng ,...   \n",
       "3  0: Qua 5 m√πa olympic m√† th√†nh_t√≠ch v·∫´n s√°ng ch...   \n",
       "4  0: H√£y tr·ªü_v·ªÅ th√¥i c√°c anh !\\n1: c·∫ßu_mong c√°c ...   \n",
       "\n",
       "                                         golden_docs  \\\n",
       "0  S√°ng 19/8 , th√†nh_ph·ªë Y√™n_B√°i m∆∞a kh√¥ng ng·ªõt d...   \n",
       "1  Chi·ªÅu 26/8 , m∆∞a to k√©o_d√†i khi·∫øn nhi·ªÅu n∆°i ·ªü ...   \n",
       "2  ·ª®ng_vi√™n t·ªïng_th·ªëng ƒë·∫£ng C·ªông_h√≤a Donald_Trump...   \n",
       "3  ·ªû n·ªôi_dung s·ªü_tr∆∞·ªùng 200 m b∆∞·ªõm , Phelps - ƒë√£ ...   \n",
       "4  L√™_Ki√™m_To√†n - ƒê·∫°i_t√° L·ªØ_ƒëo√†n tr∆∞·ªüng L·ªØ_ƒëo√†n 9...   \n",
       "\n",
       "                                      golden_comment  \\\n",
       "0  T·∫°i_sao nh∆∞_v·∫≠y ? Ph·∫£i c√≥ nguy√™n_nh√¢n , Ch√∫ng_...   \n",
       "1  B·∫øn du_thuy·ªÅn T√ÇN_S∆†N_NH·∫§T m·ªõi khai_tr∆∞∆°ng . ....   \n",
       "2  gi·ªù th√¨ t√¥i kh√¥ng c√≤n tin √¥ng ·∫•y s·∫Ω th·∫Øng , nh...   \n",
       "3  Qua 5 m√πa olympic m√† th√†nh_t√≠ch v·∫´n s√°ng ch√≥i ...   \n",
       "4  H√£y tr·ªü_v·ªÅ th√¥i c√°c anh ! c·∫ßu_mong c√°c anh tr·ªü...   \n",
       "\n",
       "                                         golden_full  \\\n",
       "0  B√≠_th∆∞ T·ªânh_u·ª∑ Y√™n_B√°i_Ph·∫°m_Duy_C∆∞·ªùng ƒë∆∞·ª£c h√†n...   \n",
       "1  C∆°n m∆∞a l·ªõn chi·ªÅu 26/8 l√†m s√¢n_bay T√¢n_S∆°n_Nh·∫•...   \n",
       "2  Donald_Trump c√°o_bu·ªôc T·ªïng_th·ªëng M·ªπ_Barack_Oba...   \n",
       "3  Si√™u k√¨nh_ng∆∞ Michael_Phelps ti·∫øp_t·ª•c cho th·∫•y...   \n",
       "4  D√¢n_tr√≠ Chi·∫øc m√°y_bay CASA ch·ªü 9 ng∆∞·ªùi ƒë√£ r∆°i ...   \n",
       "\n",
       "                               selected_docs_indices  \\\n",
       "0  [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "1  [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "2                        [0, 1, 2, 3, 4, 5, 6, 8, 9]   \n",
       "3  [0, 2, 3, 5, 6, 7, 8, 9, 10, 12, 16, 17, 18, 2...   \n",
       "4                     [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]   \n",
       "\n",
       "                            selected_comment_indices  num_docs  num_comments  \n",
       "0  [0, 1, 6, 7, 9, 11, 13, 14, 15, 17, 21, 23, 24...        26            29  \n",
       "1  [0, 1, 6, 10, 11, 12, 16, 21, 23, 24, 25, 26, ...        16           191  \n",
       "2  [0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 1...        11            37  \n",
       "3  [0, 1, 3, 4, 6, 7, 9, 10, 11, 12, 13, 14, 17, ...        24           120  \n",
       "4      [0, 1, 2, 3, 4, 5, 7, 10, 12, 13, 14, 15, 16]        11            17  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T·∫°o DataFrame v√† l∆∞u CSV\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Hi·ªÉn th·ªã th√¥ng tin DataFrame\n",
    "print(\"Th√¥ng tin DataFrame:\")\n",
    "print(df.info())\n",
    "print(f\"\\nS·ªë d√≤ng: {len(df)}\")\n",
    "print(f\"\\nC√°c c·ªôt: {df.columns.tolist()}\")\n",
    "\n",
    "# Ki·ªÉm tra xem c√≥ ƒë·ªß c·ªôt m·ªõi kh√¥ng\n",
    "required_cols = ['content_docs', 'content_comment', 'golden_docs', 'golden_comment']\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"\\n‚ö†Ô∏è C·∫¢NH B√ÅO: Thi·∫øu c√°c c·ªôt: {missing_cols}\")\n",
    "    print(\"Vui l√≤ng ch·∫°y l·∫°i Cell 2 (h√†m extract_data_from_xml) tr∆∞·ªõc!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ ƒê√£ c√≥ ƒë·ªß c√°c c·ªôt m·ªõi!\")\n",
    "\n",
    "# Xem v√†i d√≤ng ƒë·∫ßu\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc96af98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ l∆∞u file: Dataset/Done/data_full.csv\n",
      "ƒê√£ l∆∞u file: Dataset/Done/data_summary.csv\n",
      "\n",
      "T·ªïng s·ªë m·∫´u: 141\n",
      "\n",
      "C√°c c·ªôt trong file CSV:\n",
      "['title', 'content_docs', 'content_comment', 'golden_docs', 'golden_comment']\n"
     ]
    }
   ],
   "source": [
    "# L∆∞u file CSV v·ªõi c√°c c·ªôt theo y√™u c·∫ßu: title, content_docs, content_comment, golden_docs, golden_comment\n",
    "# T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥\n",
    "output_dir = \"Dataset/Done\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# L∆∞u file CSV ch√≠nh v·ªõi format m·ªõi\n",
    "df_main = df[['post_id', 'title', 'content_docs', 'content_comment', 'golden_docs', 'golden_comment']]\n",
    "df_main.to_csv(f\"{output_dir}/data_full.csv\", index=False, encoding='utf-8-sig')\n",
    "print(f\"ƒê√£ l∆∞u file: {output_dir}/data_full.csv\")\n",
    "\n",
    "# L∆∞u file CSV ƒë∆°n gi·∫£n theo format y√™u c·∫ßu (title, content_docs, content_comment, golden_docs, golden_comment)\n",
    "df_simple = df[['title', 'content_docs', 'content_comment', 'golden_docs', 'golden_comment']]\n",
    "df_simple.to_csv(f\"{output_dir}/data_summary.csv\", index=False, encoding='utf-8-sig')\n",
    "print(f\"ƒê√£ l∆∞u file: {output_dir}/data_summary.csv\")\n",
    "\n",
    "print(f\"\\nT·ªïng s·ªë m·∫´u: {len(df_simple)}\")\n",
    "print(f\"\\nC√°c c·ªôt trong file CSV:\")\n",
    "print(df_simple.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1eab6",
   "metadata": {},
   "source": [
    "## Th·ª≠ nghi·ªám v·ªõi Gemini API\n",
    "\n",
    "Ph·∫ßn n√†y s·ª≠ d·ª•ng Google Gemini API ƒë·ªÉ:\n",
    "1. Ch·ªçn c√¢u ph√π h·ª£p v·ªõi ch·ªß ƒë·ªÅ\n",
    "2. T√≥m t·∫Øt c√°c c√¢u b√¨nh lu·∫≠n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t th∆∞ vi·ªán Google Generative AI (ch·∫°y 1 l·∫ßn)\n",
    "# !pip install google-generativeai\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "# C·∫•u h√¨nh API Key - Thay YOUR_API_KEY b·∫±ng API key c·ªßa b·∫°n\n",
    "# L·∫•y API key t·∫°i: https://aistudio.google.com/app/apikey\n",
    "GEMINI_API_KEY = \"YOUR_API_KEY\"  # <-- Thay th·∫ø b·∫±ng API key c·ªßa b·∫°n\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Kh·ªüi t·∫°o model\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "print(\"ƒê√£ c·∫•u h√¨nh Gemini API th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c478a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m g·ªçi Gemini API ƒë·ªÉ ch·ªçn c√¢u\n",
    "def select_sentences_with_gemini(title, content, num_sentences=6):\n",
    "    \"\"\"\n",
    "    S·ª≠ d·ª•ng Gemini ƒë·ªÉ ch·ªçn c√°c c√¢u ph√π h·ª£p v·ªõi ch·ªß ƒë·ªÅ\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch vƒÉn b·∫£n ti·∫øng Vi·ªát. \n",
    "H√£y ch·ªçn ra {num_sentences} c√¢u ph√π h·ª£p nh·∫•t v·ªõi ch·ªß ƒë·ªÅ sau: {title}\n",
    "\n",
    "<list c√¢u>\n",
    "{content}\n",
    "</list c√¢u>\n",
    "\n",
    "Tr·∫£ v·ªÅ CH√çNH X√ÅC d∆∞·ªõi d·∫°ng danh s√°ch s·ªë, v√≠ d·ª•: [1, 2, 3, 5, 8, 10]\n",
    "Ch·ªâ tr·∫£ v·ªÅ danh s√°ch s·ªë, kh√¥ng gi·∫£i th√≠ch.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"L·ªói: {str(e)}\"\n",
    "\n",
    "# H√†m g·ªçi Gemini API ƒë·ªÉ t√≥m t·∫Øt\n",
    "def summarize_with_gemini(title, content):\n",
    "    \"\"\"\n",
    "    S·ª≠ d·ª•ng Gemini ƒë·ªÉ t√≥m t·∫Øt c√°c c√¢u b√¨nh lu·∫≠n\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"B·∫°n l√† m·ªôt chuy√™n gia t√≥m t·∫Øt vƒÉn b·∫£n ti·∫øng Vi·ªát.\n",
    "H√£y t√≥m t·∫Øt c√°c b√¨nh lu·∫≠n d∆∞·ªõi ƒë√¢y theo ch·ªß ƒë·ªÅ: {title}\n",
    "\n",
    "<list c√¢u b√¨nh lu·∫≠n>\n",
    "{content}\n",
    "</list c√¢u b√¨nh lu·∫≠n>\n",
    "\n",
    "Y√™u c·∫ßu:\n",
    "- T√≥m t·∫Øt ng·∫Øn g·ªçn, s√∫c t√≠ch\n",
    "- Gi·ªØ l·∫°i c√°c √Ω ki·∫øn quan tr·ªçng\n",
    "- Vi·∫øt b·∫±ng ti·∫øng Vi·ªát t·ª± nhi√™n\n",
    "\n",
    "Output:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"L·ªói: {str(e)}\"\n",
    "\n",
    "print(\"ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m g·ªçi Gemini API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b00f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th·ª≠ nghi·ªám v·ªõi 1 m·∫´u d·ªØ li·ªáu\n",
    "sample_idx = 0\n",
    "sample = df.iloc[sample_idx]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TH·ª¨ NGHI·ªÜM V·ªöI GEMINI API\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìå CH·ª¶ ƒê·ªÄ: {sample['title']}\")\n",
    "print(f\"\\nüìù S·ªê C√ÇU B√åNH LU·∫¨N: {sample['num_comments']}\")\n",
    "\n",
    "# 1. Ch·ªçn c√¢u v·ªõi Gemini\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"üîç TASK 1: CH·ªåN C√ÇU PH√ô H·ª¢P\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "gemini_selection = select_sentences_with_gemini(\n",
    "    sample['title'], \n",
    "    sample['content'],\n",
    "    num_sentences=6\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Gemini ch·ªçn: {gemini_selection}\")\n",
    "print(f\"üìä Ground Truth: {sample['selected_indices']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1330bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. T√≥m t·∫Øt v·ªõi Gemini\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"üìù TASK 2: T√ìM T·∫ÆT B√åNH LU·∫¨N\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "gemini_summary = summarize_with_gemini(\n",
    "    sample['title'], \n",
    "    sample['content']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Gemini t√≥m t·∫Øt:\")\n",
    "print(gemini_summary)\n",
    "\n",
    "print(f\"\\nüìä Ground Truth Summary:\")\n",
    "print(sample['golden'][:500] + \"...\" if len(sample['golden']) > 500 else sample['golden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0845949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y batch tr√™n nhi·ªÅu m·∫´u (gi·ªõi h·∫°n ƒë·ªÉ tr√°nh rate limit)\n",
    "import time\n",
    "\n",
    "def process_batch_with_gemini(df, num_samples=5, delay=1):\n",
    "    \"\"\"\n",
    "    X·ª≠ l√Ω batch v·ªõi Gemini API\n",
    "    delay: th·ªùi gian ch·ªù gi·ªØa c√°c request (gi√¢y)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx in range(min(num_samples, len(df))):\n",
    "        sample = df.iloc[idx]\n",
    "        print(f\"\\nüîÑ ƒêang x·ª≠ l√Ω m·∫´u {idx + 1}/{num_samples}: {sample['post_id']}\")\n",
    "        \n",
    "        # Ch·ªçn c√¢u\n",
    "        selection = select_sentences_with_gemini(\n",
    "            sample['title'], \n",
    "            sample['content'],\n",
    "            num_sentences=6\n",
    "        )\n",
    "        \n",
    "        # T√≥m t·∫Øt\n",
    "        summary = summarize_with_gemini(\n",
    "            sample['title'], \n",
    "            sample['content']\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'post_id': sample['post_id'],\n",
    "            'title': sample['title'],\n",
    "            'gemini_selection': selection,\n",
    "            'ground_truth_selection': sample['selected_indices'],\n",
    "            'gemini_summary': summary,\n",
    "            'ground_truth_summary': sample['golden']\n",
    "        })\n",
    "        \n",
    "        # Delay ƒë·ªÉ tr√°nh rate limit\n",
    "        if idx < num_samples - 1:\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Ch·∫°y batch (uncomment ƒë·ªÉ ch·∫°y)\n",
    "# results_df = process_batch_with_gemini(df, num_samples=3, delay=2)\n",
    "# results_df.to_csv(f\"{output_dir}/gemini_results.csv\", index=False, encoding='utf-8-sig')\n",
    "# print(\"\\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o gemini_results.csv\")\n",
    "\n",
    "print(\"üí° Uncomment d√≤ng tr√™n ƒë·ªÉ ch·∫°y batch v·ªõi Gemini API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m ƒë√°nh gi√° k·∫øt qu·∫£ (so s√°nh v·ªõi ground truth)\n",
    "import ast\n",
    "import re\n",
    "\n",
    "def parse_selection(selection_str):\n",
    "    \"\"\"Parse selection string th√†nh list s·ªë\"\"\"\n",
    "    try:\n",
    "        # T√¨m pattern [1, 2, 3, ...]\n",
    "        match = re.search(r'\\[[\\d,\\s]+\\]', selection_str)\n",
    "        if match:\n",
    "            return ast.literal_eval(match.group())\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def calculate_precision_recall_f1(predicted, ground_truth):\n",
    "    \"\"\"T√≠nh Precision, Recall, F1\"\"\"\n",
    "    predicted_set = set(predicted)\n",
    "    ground_truth_set = set(ground_truth)\n",
    "    \n",
    "    if len(predicted_set) == 0:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    tp = len(predicted_set & ground_truth_set)\n",
    "    precision = tp / len(predicted_set) if len(predicted_set) > 0 else 0\n",
    "    recall = tp / len(ground_truth_set) if len(ground_truth_set) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1\n",
    "\n",
    "# V√≠ d·ª• ƒë√°nh gi√°\n",
    "example_predicted = parse_selection(\"[1, 3, 5, 7, 9, 13]\")\n",
    "example_ground_truth = [0, 1, 6, 7, 13, 14, 15, 24, 25]\n",
    "\n",
    "p, r, f1 = calculate_precision_recall_f1(example_predicted, example_ground_truth)\n",
    "print(\"üìä V√≠ d·ª• ƒë√°nh gi√° Selection:\")\n",
    "print(f\"   Predicted: {example_predicted}\")\n",
    "print(f\"   Ground Truth: {example_ground_truth}\")\n",
    "print(f\"   Precision: {p:.2%}\")\n",
    "print(f\"   Recall: {r:.2%}\")\n",
    "print(f\"   F1 Score: {f1:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70638c7d",
   "metadata": {},
   "source": [
    "## Th·ª≠ nghi·ªám v·ªõi Groq API\n",
    "\n",
    "Groq API c√≥ t·ªëc ƒë·ªô r·∫•t nhanh v√† h·ªó tr·ª£ c√°c model nh∆∞ Llama 3.3, Mixtral, Gemma.\n",
    "- L·∫•y API key t·∫°i: https://console.groq.com/keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf68d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t th∆∞ vi·ªán Groq (ch·∫°y 1 l·∫ßn)\n",
    "# !pip install groq\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "# C·∫•u h√¨nh API Key - Thay YOUR_GROQ_API_KEY b·∫±ng API key c·ªßa b·∫°n\n",
    "# L·∫•y API key t·∫°i: https://console.groq.com/keys\n",
    "GROQ_API_KEY = \"YOUR_GROQ_API_KEY\"  # <-- Thay th·∫ø b·∫±ng API key c·ªßa b·∫°n\n",
    "\n",
    "# Kh·ªüi t·∫°o client\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# C√°c model c√≥ s·∫µn tr√™n Groq (r·∫•t nhanh!)\n",
    "# - llama-3.3-70b-versatile (m·∫°nh nh·∫•t)\n",
    "# - llama-3.1-8b-instant (nhanh nh·∫•t)\n",
    "# - mixtral-8x7b-32768\n",
    "# - gemma2-9b-it\n",
    "\n",
    "GROQ_MODEL = \"llama-3.3-70b-versatile\"  # C√≥ th·ªÉ ƒë·ªïi sang model kh√°c\n",
    "\n",
    "print(f\"ƒê√£ c·∫•u h√¨nh Groq API v·ªõi model: {GROQ_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H√†m g·ªçi Groq API ƒë·ªÉ ch·ªçn c√¢u\n",
    "def select_sentences_with_groq(title, content, num_sentences=6):\n",
    "    \"\"\"\n",
    "    S·ª≠ d·ª•ng Groq ƒë·ªÉ ch·ªçn c√°c c√¢u ph√π h·ª£p v·ªõi ch·ªß ƒë·ªÅ\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch vƒÉn b·∫£n ti·∫øng Vi·ªát. \n",
    "H√£y ch·ªçn ra {num_sentences} c√¢u ph√π h·ª£p nh·∫•t v·ªõi ch·ªß ƒë·ªÅ sau: {title}\n",
    "\n",
    "<list c√¢u>\n",
    "{content}\n",
    "</list c√¢u>\n",
    "\n",
    "Tr·∫£ v·ªÅ CH√çNH X√ÅC d∆∞·ªõi d·∫°ng danh s√°ch s·ªë, v√≠ d·ª•: [1, 2, 3, 5, 8, 10]\n",
    "Ch·ªâ tr·∫£ v·ªÅ danh s√°ch s·ªë, kh√¥ng gi·∫£i th√≠ch.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=GROQ_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω ph√¢n t√≠ch vƒÉn b·∫£n ti·∫øng Vi·ªát. Ch·ªâ tr·∫£ v·ªÅ k·∫øt qu·∫£, kh√¥ng gi·∫£i th√≠ch.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"L·ªói: {str(e)}\"\n",
    "\n",
    "# H√†m g·ªçi Groq API ƒë·ªÉ t√≥m t·∫Øt\n",
    "def summarize_with_groq(title, content):\n",
    "    \"\"\"\n",
    "    S·ª≠ d·ª•ng Groq ƒë·ªÉ t√≥m t·∫Øt c√°c c√¢u b√¨nh lu·∫≠n\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"B·∫°n l√† m·ªôt chuy√™n gia t√≥m t·∫Øt vƒÉn b·∫£n ti·∫øng Vi·ªát.\n",
    "H√£y t√≥m t·∫Øt c√°c b√¨nh lu·∫≠n d∆∞·ªõi ƒë√¢y theo ch·ªß ƒë·ªÅ: {title}\n",
    "\n",
    "<list c√¢u b√¨nh lu·∫≠n>\n",
    "{content}\n",
    "</list c√¢u b√¨nh lu·∫≠n>\n",
    "\n",
    "Y√™u c·∫ßu:\n",
    "- T√≥m t·∫Øt ng·∫Øn g·ªçn, s√∫c t√≠ch (kho·∫£ng 3-5 c√¢u)\n",
    "- Gi·ªØ l·∫°i c√°c √Ω ki·∫øn quan tr·ªçng\n",
    "- Vi·∫øt b·∫±ng ti·∫øng Vi·ªát t·ª± nhi√™n\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=GROQ_MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω t√≥m t·∫Øt vƒÉn b·∫£n ti·∫øng Vi·ªát chuy√™n nghi·ªáp.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"L·ªói: {str(e)}\"\n",
    "\n",
    "print(\"ƒê√£ ƒë·ªãnh nghƒ©a c√°c h√†m g·ªçi Groq API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b5eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th·ª≠ nghi·ªám v·ªõi Groq API - 1 m·∫´u d·ªØ li·ªáu\n",
    "sample_idx = 0\n",
    "sample = df.iloc[sample_idx]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TH·ª¨ NGHI·ªÜM V·ªöI GROQ API\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìå CH·ª¶ ƒê·ªÄ: {sample['title']}\")\n",
    "print(f\"\\nüìù S·ªê C√ÇU B√åNH LU·∫¨N: {sample['num_comments']}\")\n",
    "\n",
    "# 1. Ch·ªçn c√¢u v·ªõi Groq\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"üîç TASK 1: CH·ªåN C√ÇU PH√ô H·ª¢P (GROQ)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "groq_selection = select_sentences_with_groq(\n",
    "    sample['title'], \n",
    "    sample['content_comment'],\n",
    "    num_sentences=6\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Groq ch·ªçn: {groq_selection}\")\n",
    "print(f\"üìä Ground Truth: {sample['selected_comment_indices']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
